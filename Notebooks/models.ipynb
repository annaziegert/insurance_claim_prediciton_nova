{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis of Machine Learning Models \n",
    "\n",
    "As a risk department of an insurance company, we understand the importance of building robust predictive models to accurately assess the likelihood of claims for various insurance products. In our quest to build a better model for predicting car insurance claims within the first 6 months of insurance, we have explored multiple machine learning algorithms and techniques.\n",
    "\n",
    "This notebook presents a detailed analysis of the different models we have tried, including their strengths and weaknesses, and how they compare to our baseline model. We have experimented with various algorithms such as decision trees, random forests, logistic regression, and support vector machines, among others.\n",
    "\n",
    "Our aim is to identify the best-performing model that can help us make more accurate predictions, minimize potential losses, and ensure the stability of our cash flows. By evaluating the models based on various metrics such as AUC-ROC, precision, recall, and F1-score, we can determine which model is best suited for our specific use case.\n",
    "\n",
    "With this information, we can make data-driven decisions that can help us manage risks effectively and offer competitive prices to our customers while maintaining profitability. The second notebook serves as a comprehensive guide to our model selection process, providing insights that can be leveraged to improve our existing models and build more robust predictive models in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a custom MLPDropout classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network._stochastic_optimizers import AdamOptimizer\n",
    "from sklearn.neural_network._base import ACTIVATIONS, DERIVATIVES, LOSS_FUNCTIONS\n",
    "from sklearn.utils import shuffle, gen_batches, check_random_state, _safe_indexing\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "class MLPDropout(MLPClassifier):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100,),\n",
    "        activation=\"relu\",\n",
    "        *,\n",
    "        solver=\"adam\",\n",
    "        alpha=0.0001,\n",
    "        batch_size=\"auto\",\n",
    "        learning_rate=\"constant\",\n",
    "        learning_rate_init=0.001,\n",
    "        power_t=0.5,\n",
    "        max_iter=200,\n",
    "        shuffle=True,\n",
    "        random_state=None,\n",
    "        tol=1e-4,\n",
    "        verbose=False,\n",
    "        warm_start=False,\n",
    "        momentum=0.9,\n",
    "        nesterovs_momentum=True,\n",
    "        early_stopping=False,\n",
    "        validation_fraction=0.1,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8,\n",
    "        n_iter_no_change=10,\n",
    "        max_fun=15000,\n",
    "        dropout = None,\n",
    "    ):\n",
    "        '''\n",
    "        Additional Parameters:\n",
    "        ----------\n",
    "        dropout : float in range (0, 1), default=None\n",
    "            Dropout parameter for the model, defines the percentage of nodes\n",
    "            to remove at each layer.\n",
    "            \n",
    "        '''\n",
    "        self.dropout = dropout\n",
    "        super().__init__(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            alpha=alpha,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            power_t=power_t,\n",
    "            max_iter=max_iter,\n",
    "            shuffle=shuffle,\n",
    "            random_state=random_state,\n",
    "            tol=tol,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            momentum=momentum,\n",
    "            nesterovs_momentum=nesterovs_momentum,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_fraction=validation_fraction,\n",
    "            beta_1=beta_1,\n",
    "            beta_2=beta_2,\n",
    "            epsilon=epsilon,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            max_fun=max_fun,\n",
    "        )\n",
    "    \n",
    "    def _fit_stochastic(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        activations,\n",
    "        deltas,\n",
    "        coef_grads,\n",
    "        intercept_grads,\n",
    "        layer_units,\n",
    "        incremental,\n",
    "    ):\n",
    "        params = self.coefs_ + self.intercepts_\n",
    "        if not incremental or not hasattr(self, \"_optimizer\"):\n",
    "            if self.solver == \"sgd\":\n",
    "                self._optimizer = SGDOptimizer(\n",
    "                    params,\n",
    "                    self.learning_rate_init,\n",
    "                    self.learning_rate,\n",
    "                    self.momentum,\n",
    "                    self.nesterovs_momentum,\n",
    "                    self.power_t,\n",
    "                )\n",
    "            elif self.solver == \"adam\":\n",
    "                self._optimizer = AdamOptimizer(\n",
    "                    params,\n",
    "                    self.learning_rate_init,\n",
    "                    self.beta_1,\n",
    "                    self.beta_2,\n",
    "                    self.epsilon,\n",
    "                )\n",
    "\n",
    "        # early_stopping in partial_fit doesn't make sense\n",
    "        early_stopping = self.early_stopping and not incremental\n",
    "        if early_stopping:\n",
    "            # don't stratify in multilabel classification\n",
    "            should_stratify = is_classifier(self) and self.n_outputs_ == 1\n",
    "            stratify = y if should_stratify else None\n",
    "            X, X_val, y, y_val = train_test_split(\n",
    "                X,\n",
    "                y,\n",
    "                random_state=self._random_state,\n",
    "                test_size=self.validation_fraction,\n",
    "                stratify=stratify,\n",
    "            )\n",
    "            if is_classifier(self):\n",
    "                y_val = self._label_binarizer.inverse_transform(y_val)\n",
    "        else:\n",
    "            X_val = None\n",
    "            y_val = None\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        sample_idx = np.arange(n_samples, dtype=int)\n",
    "\n",
    "        if self.batch_size == \"auto\":\n",
    "            batch_size = min(200, n_samples)\n",
    "        else:\n",
    "            if self.batch_size < 1 or self.batch_size > n_samples:\n",
    "                warnings.warn(\n",
    "                    \"Got `batch_size` less than 1 or larger than \"\n",
    "                    \"sample size. It is going to be clipped\"\n",
    "                )\n",
    "            batch_size = np.clip(self.batch_size, 1, n_samples)\n",
    "\n",
    "        try:\n",
    "            for it in range(self.max_iter):\n",
    "                if self.shuffle:\n",
    "                    # Only shuffle the sample indices instead of X and y to\n",
    "                    # reduce the memory footprint. These indices will be used\n",
    "                    # to slice the X and y.\n",
    "                    sample_idx = shuffle(sample_idx, random_state=self._random_state)\n",
    "\n",
    "                accumulated_loss = 0.0\n",
    "                for batch_slice in gen_batches(n_samples, batch_size):\n",
    "                    if self.shuffle:\n",
    "                        X_batch = _safe_indexing(X, sample_idx[batch_slice])\n",
    "                        y_batch = y[sample_idx[batch_slice]]\n",
    "                    else:\n",
    "                        X_batch = X[batch_slice]\n",
    "                        y_batch = y[batch_slice]\n",
    "                    \n",
    "                    activations[0] = X_batch\n",
    "                    # (DROPOUT ADDITION) layer_units passed forward to help build dropout mask.\n",
    "                    batch_loss, coef_grads, intercept_grads = self._backprop(\n",
    "                        X_batch,\n",
    "                        y_batch,\n",
    "                        activations,\n",
    "                        layer_units,\n",
    "                        deltas,\n",
    "                        coef_grads,\n",
    "                        intercept_grads,\n",
    "                    )\n",
    "                    accumulated_loss += batch_loss * (\n",
    "                        batch_slice.stop - batch_slice.start\n",
    "                    )\n",
    "\n",
    "                    # update weights\n",
    "                    grads = coef_grads + intercept_grads\n",
    "                    self._optimizer.update_params(params, grads)\n",
    "\n",
    "                self.n_iter_ += 1\n",
    "                self.loss_ = accumulated_loss / X.shape[0]\n",
    "\n",
    "                self.t_ += n_samples\n",
    "                self.loss_curve_.append(self.loss_)\n",
    "                if self.verbose:\n",
    "                    print(\"Iteration %d, loss = %.8f\" % (self.n_iter_, self.loss_))\n",
    "\n",
    "                # update no_improvement_count based on training loss or\n",
    "                # validation score according to early_stopping\n",
    "                self._update_no_improvement_count(early_stopping, X_val, y_val)\n",
    "\n",
    "                # for learning rate that needs to be updated at iteration end\n",
    "                self._optimizer.iteration_ends(self.t_)\n",
    "\n",
    "                if self._no_improvement_count > self.n_iter_no_change:\n",
    "                    # not better than last `n_iter_no_change` iterations by tol\n",
    "                    # stop or decrease learning rate\n",
    "                    if early_stopping:\n",
    "                        msg = (\n",
    "                            \"Validation score did not improve more than \"\n",
    "                            \"tol=%f for %d consecutive epochs.\"\n",
    "                            % (self.tol, self.n_iter_no_change)\n",
    "                        )\n",
    "                    else:\n",
    "                        msg = (\n",
    "                            \"Training loss did not improve more than tol=%f\"\n",
    "                            \" for %d consecutive epochs.\"\n",
    "                            % (self.tol, self.n_iter_no_change)\n",
    "                        )\n",
    "\n",
    "                    is_stopping = self._optimizer.trigger_stopping(msg, self.verbose)\n",
    "                    if is_stopping:\n",
    "                        break\n",
    "                    else:\n",
    "                        self._no_improvement_count = 0\n",
    "\n",
    "                if incremental:\n",
    "                    break\n",
    "\n",
    "                if self.n_iter_ == self.max_iter:\n",
    "                    warnings.warn(\n",
    "                        \"Stochastic Optimizer: Maximum iterations (%d) \"\n",
    "                        \"reached and the optimization hasn't converged yet.\"\n",
    "                        % self.max_iter,\n",
    "                        ConvergenceWarning,\n",
    "                    )\n",
    "        except KeyboardInterrupt:\n",
    "            warnings.warn(\"Training interrupted by user.\")\n",
    "\n",
    "        if early_stopping:\n",
    "            # restore best weights\n",
    "            self.coefs_ = self._best_coefs\n",
    "            self.intercepts_ = self._best_intercepts\n",
    "    \n",
    "    def _backprop(self, X, y, activations, layer_units, deltas, coef_grads, intercept_grads):\n",
    "        \"\"\"Compute the MLP loss function and its corresponding derivatives\n",
    "        with respect to each parameter: weights and bias vectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "             \n",
    "        layer_units (DROPOUT ADDITION) : list, length = n_layers\n",
    "            The layer units of the neural net, this is the shape of the\n",
    "            Neural Net model. This is used to build the dropout mask.\n",
    "\n",
    "        deltas : list, length = n_layers - 1\n",
    "            The ith element of the list holds the difference between the\n",
    "            activations of the i + 1 layer and the backpropagated error.\n",
    "            More specifically, deltas are gradients of loss with respect to z\n",
    "            in each layer, where z = wx + b is the value of a particular layer\n",
    "            before passing through the activation function\n",
    "\n",
    "        coef_grads : list, length = n_layers - 1\n",
    "            The ith element contains the amount of change used to update the\n",
    "            coefficient parameters of the ith layer in an iteration.\n",
    "\n",
    "        intercept_grads : list, length = n_layers - 1\n",
    "            The ith element contains the amount of change used to update the\n",
    "            intercept parameters of the ith layer in an iteration.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "        coef_grads : list, length = n_layers - 1\n",
    "        intercept_grads : list, length = n_layers - 1\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        dropout_masks = None\n",
    "        \n",
    "        # Create the Dropout Mask (DROPOUT ADDITION)\n",
    "        if self.dropout != None:\n",
    "            if 0 < self.dropout < 1:\n",
    "                keep_probability = 1 - self.dropout\n",
    "                dropout_masks = [np.ones(layer_units[0])]\n",
    "                \n",
    "                # Create hidden Layer Dropout Masks\n",
    "                for units in layer_units[1:-1]:\n",
    "                    # Create inverted Dropout Mask, check for random_state\n",
    "                    if self.random_state != None:\n",
    "                        layer_mask = (self._random_state.random(units) < keep_probability).astype(int) / keep_probability\n",
    "                    else:\n",
    "                        layer_mask = (np.random.rand(units) < keep_probability).astype(int) / keep_probability\n",
    "                    dropout_masks.append(layer_mask)\n",
    "            else:\n",
    "                raise ValueError('Dropout must be between zero and one. If Dropout=X then, 0 < X < 1.')\n",
    "        \n",
    "        # Forward propagate\n",
    "        # Added dropout_makss to _forward_pass call (DROPOUT ADDITION)\n",
    "        activations = self._forward_pass(activations, dropout_masks)\n",
    "        \n",
    "        # Get loss\n",
    "        loss_func_name = self.loss\n",
    "        if loss_func_name == \"log_loss\" and self.out_activation_ == \"logistic\":\n",
    "            loss_func_name = \"binary_log_loss\"\n",
    "        loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])\n",
    "        # Add L2 regularization term to loss\n",
    "        values = 0\n",
    "        for s in self.coefs_:\n",
    "            s = s.ravel()\n",
    "            values += np.dot(s, s)\n",
    "        loss += (0.5 * self.alpha) * values / n_samples\n",
    "\n",
    "        # Backward propagate\n",
    "        last = self.n_layers_ - 2\n",
    "\n",
    "        # The calculation of delta[last] here works with following\n",
    "        # combinations of output activation and loss function:\n",
    "        # sigmoid and binary cross entropy, softmax and categorical cross\n",
    "        # entropy, and identity with squared loss\n",
    "        deltas[last] = activations[-1] - y\n",
    "        \n",
    "        # Compute gradient for the last layer\n",
    "        self._compute_loss_grad(\n",
    "            last, n_samples, activations, deltas, coef_grads, intercept_grads\n",
    "        )\n",
    "\n",
    "        inplace_derivative = DERIVATIVES[self.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(self.n_layers_ - 2, 0, -1):\n",
    "            deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)\n",
    "            inplace_derivative(activations[i], deltas[i - 1])\n",
    "            \n",
    "            self._compute_loss_grad(\n",
    "                i - 1, n_samples, activations, deltas, coef_grads, intercept_grads\n",
    "            )\n",
    "        \n",
    "        # Apply Dropout Masks to the Parameter Gradients (DROPOUT ADDITION)\n",
    "        if dropout_masks != None:\n",
    "            for layer in range(len(coef_grads)-1):\n",
    "                mask = (~(dropout_masks[layer+1] == 0)).astype(int)\n",
    "                coef_grads[layer] = coef_grads[layer] * mask[None, :]\n",
    "                coef_grads[layer+1] = (coef_grads[layer+1] * mask.reshape(-1, 1))\n",
    "                intercept_grads[layer] = intercept_grads[layer] * mask\n",
    "        \n",
    "        return loss, coef_grads, intercept_grads\n",
    "    \n",
    "    def _forward_pass(self, activations, dropout_masks=None):\n",
    "        \"\"\"Perform a forward pass on the network by computing the values\n",
    "        of the neurons in the hidden layers and the output layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        activations : list, length = n_layers - 1\n",
    "            The ith element of the list holds the values of the ith layer.\n",
    "        dropout_mask : list, length = n_layers - 1\n",
    "            The ith element of the list holds the dropout mask for the ith\n",
    "            layer.\n",
    "        \"\"\"\n",
    "        hidden_activation = ACTIVATIONS[self.activation]\n",
    "        # Iterate over the hidden layers\n",
    "        for i in range(self.n_layers_ - 1):\n",
    "            activations[i + 1] = safe_sparse_dot(activations[i], self.coefs_[i])\n",
    "            activations[i + 1] += self.intercepts_[i]\n",
    "            \n",
    "            # For the hidden layers\n",
    "            if (i + 1) != (self.n_layers_ - 1):\n",
    "                hidden_activation(activations[i + 1])\n",
    "            \n",
    "            # Apply Dropout Mask (DROPOUT ADDITION)\n",
    "            if (i + 1) != (self.n_layers_ - 1) and dropout_masks != None:\n",
    "                check1 = activations[i].copy()\n",
    "                activations[i+1] = activations[i+1] * dropout_masks[i+1][None, :]\n",
    "\n",
    "        # For the last layer\n",
    "        output_activation = ACTIVATIONS[self.out_activation_]\n",
    "        output_activation(activations[i + 1])\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Optimal threshold: 0.07\n",
      "Accuracy: 53.73%\n",
      "Precision: 9.52%\n",
      "Recall: 72.62%\n",
      "ROC-AUC: 62.52%\n",
      "F1 score: 16.84% \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv('Deep_learning/ml_gw_car_insurance_cleaned.csv')\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = df.drop('is_claim', axis=1)\n",
    "y = df['is_claim']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build neural network\n",
    "model = MLPDropout(hidden_layer_sizes=(20, 10, 5), dropout=0.015, alpha=1e-07, max_iter=300, solver='adam', activation='tanh', learning_rate='invscaling', random_state=42)\n",
    "\n",
    "# Train the ensemble model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model on test set\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "# Make predictions with optimal threshold\n",
    "y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print('\\n\\nOptimal threshold: %.2f' % optimal_threshold)\n",
    "print('Accuracy: %.2f%%' % (accuracy * 100))\n",
    "print('Precision: %.2f%%' % (precision * 100))\n",
    "print('Recall: %.2f%%' % (recall * 100))\n",
    "print('ROC-AUC: %.2f%%' % round((roc_auc * 100), 2))\n",
    "print('F1 score: %.2f%%' % (f1 * 100), '\\n\\n')\n",
    "\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.set(font_scale=1.4)  # for label size\n",
    "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 16}, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rationale Deep Learning Model\n",
    "\n",
    "Deep learning models have become increasingly popular in the insurance industry due to their ability to handle large amounts of data and make accurate predictions. These models have been particularly useful in predicting insurance claims, where accuracy is crucial in order to minimize costs for insurance companies.\n",
    "\n",
    "One common application of deep learning in insurance is the analysis of claims data to identify patterns and trends. This allows insurers to gain insights into the types of claims being made and the factors that contribute to these claims, enabling them to develop more effective risk management strategies.Deep learning models can also be used to predict the likelihood of future claims, based on a range of factors such as previous claims history, demographic information, and other relevant data. This allows insurers to take proactive measures to prevent claims from occurring or to manage them more effectively when they do occur.\n",
    "\n",
    "Overall, deep learning models have significant potential to transform the insurance industry by improving accuracy, reducing costs, and enhancing the customer experience. As the technology continues to advance, we can expect to see even more innovative applications of deep learning in insurance in the years to come.\n",
    "\n",
    "### Chosen Metric\n",
    "\n",
    "Insurance companies want to maximize recall in their prediction models because it is better to be cautious and identify all potential risks or claims even if it means a higher false positive rate. False positives may result in higher costs due to investigating claims that turn out to be false, but false negatives can be much more costly for insurance companies as they miss out on potential premiums, and may lead to customers being dissatisfied with the company's services. Therefore, maximizing recall ensures that the insurance company is identifying as many potential claims as possible and minimizing the risk of missing out on potential premium revenue.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "To build a deep learning model with optimal hyperparameters, GridSearchCV was used. GridSearchCV is a method of tuning hyperparameters for a model that exhaustively searches over a specified parameter grid to find the best hyperparameters for the model. In this case, the model used is a multilayer perceptron (MLP) neural network with dropout regularization. The MLP has three hidden layers with 20, 10, and 5 neurons respectively. The activation function used is hyperbolic tangent (tanh) and the optimizer is Adam. The learning rate is invscaling, and the random seed is set to 42.\n",
    "\n",
    "To perform GridSearchCV, a parameter grid was defined with several hyperparameters and their possible values. The hyperparameters included the size of the hidden layers, the activation function, the optimizer, the maximum number of iterations, and the regularization parameter alpha. GridSearchCV was performed with 5-fold cross-validation and recall was used as the scoring metric. After performing the search, the best hyperparameters were printed, and the model was retrained with the best hyperparameters. The resulting model was the best performing one in terms of recall and was optimized using the hyperparameters found by GridSearchCV. This method helps to optimize the performance of the model and ensures that the model is not overfitting or underfitting to the data.\n",
    "\n",
    "Since GridSearchCV takes a very long time I did not include every hyperparameter but only a few. I added the rest of the hyper parameters afterwards and observed if they improve performance of not. Furhtermore it is important to note that I adjusted the number of layers and nodes at the end to determine the best possible model. Here is the code I used to perform GridSearchCV:\n",
    "\n",
    "`param_grid = {'`\n",
    "\n",
    "    'hidden_layer_sizes': [(365, )],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'max_iter': [100, 200, 300, 400],\n",
    "    'alpha': 10.0 ** -np.arange(1, 7)\n",
    "}\n",
    "\n",
    "`grid_search = GridSearchCV(model, param_grid, cv=5, scoring='recall')`\n",
    "`grid_search.fit(X_train, y_train)`\n",
    "\n",
    "`print('Best hyperparameters:', grid_search.best_params_)`\n",
    "\n",
    "`model = grid_search.best_estimator_'`\n",
    "\n",
    "\n",
    "### Model Result\n",
    "\n",
    "The given deep learning model for predicting car insurance claims seems to have a high recall of 72.62%, indicating that it correctly identifies 72.62% of the actual positive cases. However, the precision of the model is low at 9.52%, indicating that out of all the predicted positive cases, only 9.52% are actually true positives. The ROC-AUC score of 62.52% indicates that the model is better than a random classifier, but it is not very good at distinguishing between positive and negative cases. The F1 score of 16.84% is also low, indicating that the model does not have a good balance between precision and recall. The optimal threshold of 0.07 suggests that the model has a high false positive rate, as the threshold is set lower than the default 0.5. This means that the model is predicting more positive cases than it should, which may result in more false positives.\n",
    "\n",
    "Overall, the performance of the model is not very good and it may not be reliable for making accurate predictions. It is recommended to look for a better model with higher precision and F1 score.\n",
    "\n",
    "NOTE: By finetuning the hyperparameters it is possible to achieve a higher recall rate but always at cost of the overall accuracy of the model. To achieve similar results it is also possible to use the following models:\n",
    "\n",
    "`(10, 6, 3) with random_state: 12, (9, 5, 3, 2),  (20, 10, 5) with dropout=0.01, (20, 10, 5) with dropout=0.015, (20, 10, 5) with dropout=0.02.`\n",
    "\n",
    "### Link to Business Problem\n",
    "\n",
    "This output indicates that the model is able to correctly identify a significant proportion of potential insurance claims (recall), but at the cost of a relatively low accuracy and precision. While the model may identify many legitimate claims, it also identifies a large number of false positives, which could lead to unnecessary payouts and ultimately negatively impact the profitability of the insurance company. The ROC-AUC score indicates that the model's ability to distinguish between positive and negative cases is slightly better than random chance. Therefore, the insurance company may want to consider using this model as a starting point and continue to improve it to achieve better accuracy and precision while maintaining a high recall rate.\n",
    "\n",
    "\n",
    "### Challenges - Size of dataset and imbalances\n",
    "\n",
    "The adequacy of a dataset for training a deep learning model depends on various factors, such as the complexity of the problem, the number of features, the model architecture, and the amount of variation in the data. A dataset with 58592 rows x 43 columns may be sufficient for some relatively simple problems, but in general, it may not be sufficient to train a deep learning model with high accuracy and generalization ability.\n",
    "\n",
    "Firstly, deep learning models require a large amount of data to learn the underlying patterns and relationships in the data. With only 58592 rows, the model may not have enough examples to capture the complexity of the problem, especially if the data has high variation or noise. Secondly, 43 columns may not be sufficient to capture all the relevant features in the data, especially for complex tasks. Deep learning models can learn complex representations of data by using multiple layers of artificial neurons. To learn complex representations, they typically require a large number of features or a high-dimensional input space. Furthermore, if the dataset is imbalanced, meaning the number of examples in each class is not balanced, it may affect the performance of the model. Deep learning models can be sensitive to class imbalance, and a small number of examples for a particular class may lead to poor performance in predicting that class.\n",
    "\n",
    "In conclusion, the size and quality of the dataset are critical factors in training a deep learning model. While a dataset with 58592 rows x 43 columns may be sufficient for some problems, it may not be sufficient for complex tasks, and a larger and more diverse dataset may be required to achieve high accuracy and generalization performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
